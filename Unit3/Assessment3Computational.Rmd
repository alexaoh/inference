---
title: "Computational Part of Assessment Exercises"
subtitle: "Advanced Statistical Inference - Unit 3"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    theme: readable
    highlight: textmate
    number_sections: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
setwd("/home/ajo/gitRepos/inference/Unit2")
library(dplyr)
```

## Problem 2

### 2.4

Contour plot of the relative log-likelihood $\tilde{l}(\lambda, \theta)$.

```{r}
# Using the contour function. 
```


## Problem 3

### 3.4

The Newton-Raphson algorithm is implemented using the expressions obtained analytically. The function is given in the cell below. 

```{r}
newton.raphson <- function(sample, it0, eps, maxiter){
  n <- length(sample)
  d1 <- function(iterate){
    return(n/iterate + sum(log(sample)*(1-sample^iterate)))
  }
  
  d2 <- function(iterate){
    return(-n/iterate^2 - sum(sample^iterate*(log(sample))^2))
  }
  
  errest <- 2*eps
  i <- 0
  while (errest > eps && i < maxiter){
    it1 <- it0 - d1(it0)/d2(it0)
    errest <- abs(it1 - it0)
    it0 <- it1
    i <- i + 1
  }
  
  return(cbind("Fixed Point" = it1, "Iterations" = i, "d2(fixed point)" = d2(it1)))
}
```

### 3.5 

Generate samples of size n = 5 from a Weibull distribution with parameters $\alpha = 1$, $\alpha = 2$ and $\alpha = 5$ respectively. Apply the preceding algorithm to obtain a numerical approximation to the value  of $\alpha$ in each case. Comment on the difference between the results of each case.

```{r}
set.seed(1)
n <- 5
alpha_1 <- rweibull(n = n, scale = 1, shape = 1)
alpha_2 <- rweibull(n = n, scale = 1, shape = 2)
alpha_5 <- rweibull(n = n, scale = 1, shape = 5)
```

The numerical approximation in each of the cases is given below. First, the numerical approximation to $\alpha = 1$ when generating $n = 5$ samples is

```{r}
eps <- 0.0000000000001
par <- 0.5
newton.raphson(alpha_1, par, eps, 100)
```

Next, the numerical approximation to $\alpha = 2$ when generating $n = 5$ samples is

```{r}
newton.raphson(alpha_2, par, eps, 100)
```

Finally, the numerical approximation to $\alpha = 5$ when generating $n = 5$ samples is

```{r}
newton.raphson(alpha_5, par, eps, 100)
```

It is apparent that none of the results are very similar to the true values of $\alpha$. This is largely because we only have $n = 5$ samples in each case. If the sample size is increased, the fixed points come much closer to the true values. 

OTHER COMMENTS?!

For completeness we check the results using the `optim` function. The first results is 

```{r}
loglik <- function(alpha, data){
  n <- length(data)
  (n*log(alpha) - sum(data^alpha) + (alpha-1)*sum(log(data)))
}

gr <- function(alpha, data){
  n <- length(data)
  return(n/alpha + sum(log(data)*(1-data^alpha)))
}

# optim(par = 0.5, fn = loglik, data = alpha_1, 
#       hessian = T, method = "L-BFGS-B", lower = eps, 
#       upper = 1-eps, control = list(fnscale = -1))

res1 <- optim(par = par, fn = loglik, data = alpha_1, 
      hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res1$par, "Iterations" = res1$counts[[1]], "Hessian" = res1$hessian[[1]], "Convergence" = res1$convergence))

res1 <- optim(par = par, fn = loglik, gr = gr, data = alpha_1, hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res1$par, "Iterations" = res1$counts[[1]], "Hessian" = res1$hessian[[1]], "Convergence" = res1$convergence))

# The below gives the same result. 
optimize(loglik, data = alpha_1, interval = c(0,10), maximum = T)
```

The second result is 

```{r}
res2 <- optim(par = par, fn = loglik, data = alpha_2, 
      hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res2$par, "Iterations" = res2$counts[[1]], "Hessian" = res2$hessian[[1]], "Convergence" = res2$convergence))

res2 <- optim(par = par, fn = loglik, gr = gr, data = alpha_2, 
      hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res2$par, "Iterations" = res2$counts[[1]], "Hessian" = res2$hessian[[1]], "Convergence" = res2$convergence))
```

The third result is 

```{r}
res3 <- optim(par = par, fn = loglik, data = alpha_5, 
      hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res3$par, "Iterations" = res3$counts[[1]], "Hessian" = res3$hessian[[1]], "Convergence" = res3$convergence))

res3 <- optim(par = par, fn = loglik, gr = gr, data = alpha_5, 
      hessian = T, control = list(fnscale = -1))
(cbind("Fixed Point" = res3$par, "Iterations" = res3$counts[[1]], "Hessian" = res3$hessian[[1]], "Convergence" = res3$convergence))
```

As is apparent, the `optim` function (using the Nelder-Mead method for maximization) gives a very similar results to our implementation. 

## Problem 4

Consider 80 observations from a multinomial model with three cells governed by a parameter $p \in (0,.2)$

| Cell        | 1         | 2         | 3          |
|-------------|-----------|-----------|------------|
| Probability | $p + 0.2$ | $p + 0.4$ | $0.4 - 2p$ |
| Frequencies | 20        | 50        | 10         |

Use the EM algorithm to approximate the MLE of $p$. Take $p_0 = 0.1$ as the initial value. 

First of all I will calculate the explicit log-likelihood function for this problem and maximize it. After this, we will see that the EM algorithm approximates the result.  

```{r}
y1 <- 20
y2 <- 50
y3 <- 10
```

Since we know that this is a multinomial model, the joint density of the observations is 

$$
f(\mathbf{y}|p) = \frac{n!}{y_1!y_2!y_3!} (p+0.2)^{y_1}(p+0.4)^{y_2}(0.4-2p)^{y_3}.
$$

Thus, the log-likelihood function is 

$$
l(p|\mathbf{y}) = y_1\log(p+0.2) + y_2\log(p+0.4) + y_3\log(0.4-2p),
$$

when we ignore the values that do not depend on $p$. This can be maximized analytically, which requires some tedious computation. Instead, it is maximized numerically, via the `optimize` function. The result is 

```{r}
# Maximize the analytical function (numerically) to check results. 
analytic.lik <- function(p){
  return(y1*log(p+0.2) + y2*log(p+0.4)+y3*log(0.4-2*p))
}
ana <- optimize(analytic.lik, interval = c(0,0.2), maximum = T)
ana$maximum
```

where the value of the objective function in the maximum is `r ana$objective`. As inspired by some [lecture notes](https://www.math.kth.se/matstat/gru/Statistical%20inference/Lecture8.pdf) from KTH on the EM algorithm, let us imagine that we have four cells instead of three by splitting the first cell into two. Thus, we have the table 

| Cell        | 1.1       | 1.2       | 2          | 3          |
|-------------|-----------|-----------|------------|------------|
| Probability | $p$       | $0.2$     | $p + 0.4$  | $0.4 - 2p$ |
| Frequencies | ?         | ?         | 50         | 10         |

where we no longer know the exact frequency in cell 1.1 and 1.2. We only know that $y_{1.1} + y_{1.2} = y_1 = 20$. The joint density with this model in mind is 

$$
f_2(\mathbf{y}|p) = \frac{n!}{y_{1.1}!y_{1.2}!y_2!y_3!} p^{y_{1.1}}0.2^{y_{1.2}}(p+0.4)^{y_2}(0.4-2p)^{y_3}, 
$$

which gives the log-likelihood function

$$
l_2(p|\mathbf{y}) = y_{1.1}\log(p) + y_2\log(p+0.4) + y_3\log(0.4-2p).
$$

This cannot be maximized, since $y_{1.1}$ is a latent variable. This is where the EM algorithm comes into play. Set the initial guess $p^{(0)} = 0.1$. In the first $\textit{expectation}$-step (E-step) we calculate

$$
Q(p|p^{(0)}) = \text{E}_{p^{(0)}}[l_2(p|\mathbf{Y})|Y_1, Y_2, Y_3], 
$$

i.e. the expectation of the log-likelihood function given the observed variables, with respect to the initial guess for the parameter $p$. In the $\textit{maximization}$-step (M-step) the guess for $p$ will be updated by maximizing the expectation found in the E-step. Going back to the E-step, the value is calculated by noting that $Y_{1.1}|Y_1 \sim Bin(n = Y_1, p = \frac{p^{(0)}}{p^{(0)}+0.2})$ for the first step. Thus, we get 

$$
\begin{align}
  Q(p|p^{(0)}) &= \text{E}_{p^{(0)}}[l_2(p|\mathbf{Y})|Y_1, Y_2, Y_3] \\
               &= \text{E}_{p^{(0)}}[Y_{1.1}|Y_1]\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p) \\ 
               &= \frac{Y_1p^{(0)}}{p^{(0)} + 0.2}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p) \\
               &= y_{1.1}^{(0)}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p),
\end{align}
$$

where we have defined $y_{1.1}^{(0)} := \frac{Y_1p^{(0)}}{p^{(0)} + 0.2}$. As already noted, the expectation $Q(p|p^{(0)})$ is maximized with respect to $p$ in the M-step, in order to find the new guess for $p$, $p^{(1)}$. The final iteration takes the form 

$$
\begin{align}
y_{1.1}^{(i)} &= \frac{Y_1p^{(i)}}{p^{(i)} + 0.2},\\
p^{(i+1)} &= \max_{p}Q(p|p^{(i)}) = \max_{p} y_{1.1}^{(i)}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p).
\end{align}
$$

This is what is implemented in R, to approximate the MLE of $p$. As for the maximization done in the first case, it is done via the function `optimize` here as well, since analytic maximization is tedious. The iteration is stopped when the difference between consecutive values of $p^{(i)}$ reaches some (low) tolerance level. The results are shown below. 

```{r}
p0 <- 0.1 # Initial value

# E-step.
E <- function(p0){
    return((y1*p0)/(p0+0.2))
}
# M-step.
M <- function(p0, expe){
  l <- function(p){
    return(expe*log(p)+y2*log(p+0.4)+y3*log(0.4-2*p))
  }
  m <- optimize(l, interval = c(0,0.2), maximum = T)
  return(m$maximum)
}

# Fixed point iterations.
EM <- function(p0, maxiter, tol){
  errest <- 2*tol
  i <- 0
  ps <- c(p0)
  while (errest > tol && i < maxiter){
    expe <- E(p0)
    p1 <- M(p0, expe)
    ps <- c(ps, p1)
    errest <- abs(p1-p0)
    p0 <- p1
    i <- i + 1
  }
  return(list(ps, p1, i))
}

maxiter <- 100
tol <- 0.000000000000001
results <- EM(p0, maxiter, tol)

knitr::kable(data.frame("Value" = results[[1]]), row.names = 1:results[[3]], caption = "Values of p in each Iteration")
```

As we can see, the approximated value to the MLE of $p$ is `r results[[2]]`, and the objective function at this value is `r analytic.lik(results[[2]])`. Comparing with the first solution, as shown in the table below, we see that they are very similar. 

```{r}
df <- cbind("First Max." = c(ana$maximum, ana$objective), "EM" = c(results[[2]], analytic.lik(results[[2]])))
rownames(df) <- c("MLE approx. of p", "Objective Value")
knitr::kable(df, caption = "Comparison of Analytic Maximization and EM")
```

