---
title: "Computational Part of Assessment Exercises"
subtitle: "Advanced Statistical Inference - Unit 3"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    theme: readable
    highlight: textmate
    number_sections: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, warning = F)
setwd("/home/ajo/gitRepos/inference/Unit2")
library(dplyr)
```

## Problem 2

### 2.4

The profile log-likelihoods $l_p(\lambda)$ and $l_p(\theta)$ are plotted below. 

```{r}
d1 <- 17
d2 <- 28
d <- d1+d2
Y1 <- 2768.9
Y2 <- 1857.5
n <- 1000
```

First, $l_p(\lambda)$ is plotted. 

```{r}
lambda <- seq(0,30, length.out = n)
l.lambda <- function(lambda){
  d1*log(lambda)-lambda*Y1
}
plot(lambda, l.lambda(lambda), ty = "l", main = "Profile log-likelihood for lambda", col = "red")
```

Next, $l_p(\theta)$ is plotted. 

```{r}
theta <- seq(0, 30, length.out = n)
l.theta <- function(theta){
  -d*log(theta*Y2 + Y1) + d2*log(theta)
}
plot(theta, l.theta(theta), ty = "l", main = "Profile log-likelihood for theta", col = "red", lty = 1)
```

A contour plot of the relative log-likelihood $\tilde{l}(\lambda, \theta)$ is made below. 

```{r}
loglik.prob2 <- function(lambda, theta){
  d*log(lambda)+d2*log(theta)-lambda*Y1-theta%o%lambda*Y2 
}
lambda.hat <- rep(d1/Y1, length.out = n)
theta.hat <- rep(d2/d1*Y1/Y2, length.out = n)

relative.like <- function(lambda, theta){
  loglik.prob2(lambda, theta)/loglik.prob2(lambda.hat, theta.hat)
}

contour(x = lambda, y = theta, z = relative.like(lambda, theta), main = "Relative log-likelihood")
lines(x = lambda, y = theta.hat, col = "red", lty = 3)
lines(x = lambda.hat, y = theta, col = "blue", lty = 2)
```

The red dotted line represents $(\lambda, \hat{\theta}(\lambda))$ and the blue dotted line represents $(\hat{\lambda}(\theta), \theta)$.

## Problem 3

### 3.4

The Newton-Raphson algorithm is implemented using the expressions obtained analytically.  

```{r}
newton.raphson <- function(sample, it0, eps, maxiter){
  n <- length(sample)
  d1 <- function(iterate){
    # First derivative of log-likelihood.
    return(n/iterate + sum(log(sample)*(1-sample^iterate)))
  }
  
  d2 <- function(iterate){
    # Second derivative of log-likelihood.
    return(-n/iterate^2 - sum(sample^iterate*(log(sample))^2))
  }
  
  errest <- 2*eps
  i <- 0
  while (errest > eps && i < maxiter){
    it1 <- it0 - d1(it0)/d2(it0)
    errest <- abs(it1 - it0)
    it0 <- it1
    i <- i + 1
  }
  
  return(cbind("Fixed Point" = it1, "Iterations" = i, "d2(fixed point)" = d2(it1)))
}
```

### 3.5 

Generate samples of size n = 5 from a Weibull distribution with parameters $\alpha = 1$, $\alpha = 2$ and $\alpha = 5$ respectively. Apply the preceding algorithm to obtain a numerical approximation to the value  of $\alpha$ in each case. Comment on the difference between the results of each case.

The numerical approximation in each of the cases is given in the table below. 

```{r}
set.seed(1)
n <- 5
alpha_1 <- rweibull(n = n, scale = 1, shape = 1)
alpha_2 <- rweibull(n = n, scale = 1, shape = 2)
alpha_5 <- rweibull(n = n, scale = 1, shape = 5)
eps <- 0.0000000000001
par <- 0.5
s.1 <- newton.raphson(alpha_1, par, eps, 100)
s.2 <- newton.raphson(alpha_2, par, eps, 100)
s.5 <- newton.raphson(alpha_5, par, eps, 100)
results <- rbind(s.1, s.2, s.5)
rownames(results) <- c("alpha = 1", "alpha = 2", "alpha = 5") 
knitr::kable(results, caption = "Approx. With Newton-Raphson")
```

It is apparent that none of the results are very similar to the true values of $\alpha$. This is largely because we only have $n = 5$ samples in each case. If the sample size is increased, the fixed points come much closer to the true values. 

An example of the results after an increased sample size ($n=10000$) can be seen below. 

```{r}
set.seed(1)
n <- 10000
alpha_1.1 <- rweibull(n = n, scale = 1, shape = 1)
alpha_2.1 <- rweibull(n = n, scale = 1, shape = 2)
alpha_5.1 <- rweibull(n = n, scale = 1, shape = 5)
s.bigger.1 <- newton.raphson(alpha_1.1, par, eps, 100)
s.bigger.2 <- newton.raphson(alpha_2.1, par, eps, 100)
s.bigger.5 <- newton.raphson(alpha_5.1, par, eps, 100)
results <- rbind(s.bigger.1, s.bigger.2, s.bigger.5)
rownames(results) <- c("alpha = 1", "alpha = 2", "alpha = 5") 
knitr::kable(results, caption = "Approx. After Increasing Sample Size")
```

For completeness we check the results, when generating samples of size $n=5$, using the `optim` function. The results are 

```{r}
loglik <- function(alpha, data){
  n <- length(data)
  (n*log(alpha) - sum(data^alpha) + (alpha-1)*sum(log(data)))
}

gr <- function(alpha, data){
  n <- length(data)
  return(n/alpha + sum(log(data)*(1-data^alpha)))
}

res1 <- optim(par = par, fn = loglik, data = alpha_1, 
      hessian = T, control = list(fnscale = -1))

res2 <- optim(par = par, fn = loglik, data = alpha_2, 
      hessian = T, control = list(fnscale = -1))

res3 <- optim(par = par, fn = loglik, data = alpha_5, 
      hessian = T, control = list(fnscale = -1))
results <- rbind(c(res1$par, res1$convergence, res1$hessian), 
                 c(res2$par, res2$convergence, res2$hessian),
                 c(res3$par, res3$convergence, res3$hessian))
rownames(results) <- c("alpha = 1", "alpha = 2", "alpha = 5") 
colnames(results) <- c("Fixed Point", "Convergence", "Hessian")
knitr::kable(results, caption = "Approx. With Optim.")
```

As is apparent, the `optim` function (using the Nelder-Mead method for maximization) gives very similar results to our implementation. 

## Problem 4

Consider 80 observations from a multinomial model with three cells governed by a parameter $p \in (0,.2)$

| Cell        | 1         | 2         | 3          |
|-------------|-----------|-----------|------------|
| Probability | $p + 0.2$ | $p + 0.4$ | $0.4 - 2p$ |
| Frequencies | 20        | 50        | 10         |

Use the EM algorithm to approximate the MLE of $p$. Take $p_0 = 0.1$ as the initial value. 

First of all I will calculate the explicit log-likelihood function for this problem and maximize it. After this, we will see that the EM algorithm approximates the result.  

```{r}
y1 <- 20
y2 <- 50
y3 <- 10
```

Since we know that this is a multinomial model, the joint density of the observations is 

$$
f(\mathbf{y}|p) = \frac{n!}{y_1!y_2!y_3!} (p+0.2)^{y_1}(p+0.4)^{y_2}(0.4-2p)^{y_3}.
$$

Thus, the log-likelihood function is 

$$
l(p|\mathbf{y}) = y_1\log(p+0.2) + y_2\log(p+0.4) + y_3\log(0.4-2p),
$$

when we ignore the factors that do not depend on $p$. This can be maximized analytically, which requires some tedious computation. Instead, it is maximized numerically, via the `optimize` function. The result is 

```{r}
# Maximize the analytical function (numerically) to check results. 
analytic.lik <- function(p){
  return(y1*log(p+0.2) + y2*log(p+0.4)+y3*log(0.4-2*p))
}
ana <- optimize(analytic.lik, interval = c(0,0.2), maximum = T)
ana$maximum
```

where the value of the objective function in the maximum is `r ana$objective`. As inspired by some [lecture notes](https://www.math.kth.se/matstat/gru/Statistical%20inference/Lecture8.pdf) from KTH on the EM algorithm, let us imagine that we have four cells instead of three by splitting the first cell into two. Thus, we have the table 

| Cell        | 1.1       | 1.2       | 2          | 3          |
|-------------|-----------|-----------|------------|------------|
| Probability | $p$       | $0.2$     | $p + 0.4$  | $0.4 - 2p$ |
| Frequencies | ?         | ?         | 50         | 10         |

where we no longer know the exact frequency in cell 1.1 and 1.2. We only know that $y_{1.1} + y_{1.2} = y_1 = 20$. The joint density with this model in mind is 

$$
f_2(\mathbf{y}|p) = \frac{n!}{y_{1.1}!y_{1.2}!y_2!y_3!} p^{y_{1.1}}0.2^{y_{1.2}}(p+0.4)^{y_2}(0.4-2p)^{y_3}, 
$$

which gives the log-likelihood function

$$
l_2(p|\mathbf{y}) = y_{1.1}\log(p) + y_2\log(p+0.4) + y_3\log(0.4-2p).
$$

This cannot be maximized, since $y_{1.1}$ is a latent variable. This is where the EM algorithm comes into play. Set the initial guess $p^{(0)} = 0.1$. In the first $\textit{expectation}$-step (E-step) we calculate

$$
Q(p|p^{(0)}) = \text{E}_{p^{(0)}}[l_2(p|\mathbf{Y})|Y_1, Y_2, Y_3], 
$$

i.e. the expectation of the log-likelihood function given the observed variables, with respect to the initial guess for the parameter $p$. In the $\textit{maximization}$-step (M-step) the guess for $p$ will be updated by maximizing the expectation found in the E-step. Going back to the E-step, the value is calculated by noting that $Y_{1.1}|Y_1 \sim Bin(n = Y_1, p = \frac{p^{(0)}}{p^{(0)}+0.2})$ for the first step. Thus, we get 

$$
\begin{align}
  Q(p|p^{(0)}) &= \text{E}_{p^{(0)}}[l_2(p|\mathbf{Y})|Y_1, Y_2, Y_3] \\
               &= \text{E}_{p^{(0)}}[Y_{1.1}|Y_1]\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p) \\ 
               &= \frac{Y_1p^{(0)}}{p^{(0)} + 0.2}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p) \\
               &= y_{1.1}^{(0)}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p),
\end{align}
$$

where we have defined $y_{1.1}^{(0)} := \frac{Y_1p^{(0)}}{p^{(0)} + 0.2}$. As already noted, the expectation $Q(p|p^{(0)})$ is maximized with respect to $p$ in the M-step, in order to find the new guess for $p$, $p^{(1)}$. The final iteration takes the form 

$$
\begin{align}
y_{1.1}^{(i)} &= \frac{Y_1p^{(i)}}{p^{(i)} + 0.2},\\
p^{(i+1)} &= \max_{p}Q(p|p^{(i)}) = \max_{p} y_{1.1}^{(i)}\log(p) + Y_2\log(p+0.4)+Y_3\log(0.4-2p).
\end{align}
$$

This is what is implemented in R, to approximate the MLE of $p$. As for the maximization done in the first case, it is done via the function `optimize` here as well, since analytic maximization is tedious. The iteration is stopped when the difference between consecutive values of $p^{(i)}$ reaches some (low) tolerance level. The results are shown below. 

```{r}
p0 <- 0.1 # Initial value

# E-step.
E <- function(p0){
    return((y1*p0)/(p0+0.2))
}
# M-step.
M <- function(p0, expe){
  l <- function(p){
    return(expe*log(p)+y2*log(p+0.4)+y3*log(0.4-2*p))
  }
  m <- optimize(l, interval = c(0,0.2), maximum = T)
  return(m$maximum)
}

# Fixed point iterations.
EM <- function(p0, maxiter, tol){
  errest <- 2*tol
  i <- 0
  ps <- c(p0)
  while (errest > tol && i < maxiter){
    expe <- E(p0)
    p1 <- M(p0, expe)
    ps <- c(ps, p1)
    errest <- abs(p1-p0)
    p0 <- p1
    i <- i + 1
  }
  return(list(ps, p1, i))
}

maxiter <- 100
tol <- 0.000000000000001
results <- EM(p0, maxiter, tol)

knitr::kable(data.frame("Value" = results[[1]]), row.names = 1:results[[3]], caption = "Values of p in each Iteration")
```

As we can see, the approximated value to the MLE of $p$ is `r results[[2]]`, and the objective function at this value is `r analytic.lik(results[[2]])`. Comparing with the first solution, as shown in the table below, we see that they are very similar. 

```{r}
df <- cbind("First Max." = c(ana$maximum, ana$objective), "EM" = c(results[[2]], analytic.lik(results[[2]])))
rownames(df) <- c("MLE approx. of p", "Objective Value")
knitr::kable(df, caption = "Comparison of Analytic Maximization and EM")
```

