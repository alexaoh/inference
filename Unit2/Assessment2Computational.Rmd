---
title: "Computational Part of Assessment Exercises"
subtitle: "Advanced Statistical Inference - Unit 2"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    theme: readable
    highlight: textmate
    number_sections: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/ajo/gitRepos/inference/Unit2")
library(bootstrap)
library(dplyr)
library(boot)
library(resample)
library(perm)
library(dgof)
library(fitdistrplus)
library(goftest)
```

## Problem 1

Modelling service times is one area of research in queuing theory. We are given a data set with service times for 174 customers. The distribution of service times is modelled using the one parameter Weibull distribution, which has the pdf

\begin{equation}
  f(x) = \frac{2x}{\theta}e^{-x^2/\theta}I_{(0,\infty)}(x). 
\end{equation}

### a) 

A plot of the empirical cdf overimposed on the theoretical distribution function is shown below. The empirical cdf is plotted together with four differentWeibull distribution functions. In all four cases, the shape parameter is fixed at 2. The scale parameters used in each case are the maximum likelihood estimator (MLE) for $\theta$, the method of moments estimator (MME) for $\theta$, a scale parameter found by a 'maximum goodness-of-fit estimation' and a scale parameter found by maximum likelihood estimation, both the latter scale parameters found with the function `fitdist`. Note that the MLE and the MME are calculated analytically in the latter parts of this exercise.

```{r, results = "hide"}
data <- read.csv("Service.csv") 
str(data)
head(data)
summary(data)
```


```{r, warning = F}
x <- seq(0, max(data$Times), by = 0.001) 
Emp.CDF <- ecdf(data$Times)
plot.ecdf(data$Times) 
MLE <- mean(data$Times^2) # Estimator found later in problem. 
MME <- 4*mean(data$Times)^2/pi # Estimator found later in problem. 
lines(x, pweibull(x, shape = 2, scale = sqrt(MLE)), lty = 2, col = "blue", lwd = 3) # MLE estimator for theta.
lines(x, pweibull(x, shape = 2, scale = sqrt(MME)), lty = 3, col = "red", lwd = 3) # MOM-estimator for theta. 

# Fix the shape parameter at 2 and only estimate the scale parameter. 
fit.weibull.mge <- fitdist(data$Times, distr = "weibull", fix.arg = list("shape" = 2), method = "mge") 
fit.weibull.mle <- fitdist(data$Times, distr = "weibull", fix.arg = list("shape" = 2), method = "mle") 
lines(x, pweibull(x, shape = 2, scale = fit.weibull.mge$estimate[[1]]), 
      lty = 4, col = "green", lwd = 3) 
lines(x, pweibull(x, shape = 2, scale = fit.weibull.mle$estimate[[1]]), 
      lty = 5, col = "orange", lwd = 3) 
legend("topleft", legend = c("MLE", "MME", "fitMGE", "fitMLE"), lty = c(2,3,4,5), col = c("blue", "red", "green", "orange"))
```

Note that the 'fitMLE' and the 'MLE' graphs are overimposed, which shows that the analytically calculated maximum likelihood estimator (later in the problem) should be correct. Initially, without conducting any goodness-of-fit tests, the different theoretical distributions look to fit the data relatively well. Also note that there are ties in the data set, i.e. that several of the values appear several times, which does not make sense to be generated by a continuous distribution. This is because the probability of a random variable $X$ taking a value $c$ is zero in the continuous case, i.e. $P(X = c) = 0$. 

### b) 

A goodness-of-fit test is done to test whether the Weibull distribution is an adequate model for these data. 

The Kolmogorov-Smirnov (KS) test is used for goodness-of-fit of each of the Weibull distribution functions found above. In the One-Sample KS test, which is used in this case, the null hypothesis states that the sample is drawn from the Weibull distribution. There are some problems with this methodology which I would like to point out. Firstly, we have the problem of ties in the data set, as already mentioned. This leads to some error in the calculations of the test-statistic and $p$-values. Secondly, the KS test does not give entirely correct results when the parameters of the theoretical distribution are estimated from the sample. The test statistic would need to be modified to correct for these mistakes. One possible solution, based on the bootstrap, can be found in a [paper](https://core.ac.uk/download/pdf/291565893.pdf) by Babu and Rao (2004). The `ks.test` function from the `dgof` library will be used to perform the test. 

The CramÃ©r-von Mises (CvM) test for goodness-of-fit is an alternative to the KS test. In the One-Sample version of the test, the null hypothesis is the same as in the KS test, that the data comes from the theoretical Weibull distribution. The `cvm.test` from the `goftest` library will be used to the perform the test. The function has an option where one can indicate whether or not the parameters are estimated from the sample, in order to adjust for the effect of this estimation. However, when this option is used, it assumes that both the parameters of the two-parameter Weibull are estimated, even though we only estimate one parameter in our case. 

```{r, warning=FALSE}
(MLE.gof <- ks.test(data$Times, "pweibull", shape = 2, scale = sqrt(MLE)))
(MME.gof <- ks.test(data$Times, "pweibull", shape = 2, scale = sqrt(MME)))

set.seed(1)
(cvm.MLE <- cvm.test(data$Times, "pweibull", shape = 2, scale = sqrt(MLE), estimated = T))
set.seed(1)
(cvm.MME <- cvm.test(data$Times, "pweibull", shape = 2, scale = sqrt(MME), estimated = T))
```

Choosing a significance level of 0.05 on the $p$-value of the tests, the One-Sample KS test would reject the null-hypothesis in the case where the MLE is used, but not in the case where MME is used. Thus, we can conclude that the Weibull with the scale parameter estimated with MLE is not a good fit, i.e. that the sample does not come from this distribution. On the other hand, we cannot conclude if the Weibull with the MME is a good fit, but it is not disregarded as a possibility at least, since the null-hypothesis is not rejected in this case. 

The One-Sample CvM test does not reject any of the null hypotheses in this case, when adjusting for the estimation of the parameters from the sample.

The goodness-of-fit measures that are calculated when using the `fitdist`-function are given below. 

```{r, warning= F}
summary(fit.weibull.mge)
(ks.MGE <- ks.test(data$Times, "pweibull", shape = 2, scale = fit.weibull.mge$estimate[[1]]))
set.seed(1)
(cvm.MGE <- cvm.test(data$Times, "pweibull", shape = 2, scale = fit.weibull.mge$estimate[[1]], estimated = T))
```

```{r, warning= F}
summary(fit.weibull.mle)
(ks.MLE <- ks.test(data$Times, "pweibull", shape = 2, scale = fit.weibull.mle$estimate[[1]])) # Same as above. 
set.seed(1)
(cvm.MLE <- cvm.test(data$Times, "pweibull", shape = 2, scale = fit.weibull.mle$estimate[[1]], estimated = T)) # Same as above.
```

When comparing the two different estimated distributions found with the `fitdist`-function, one would choose the latter model, i.e. the model fitted with the maximum likelihood, since this model has a larger loglikelihood and smaller AIC and BIC. This only makes sense when looking only at these measures. As noted earlier, the Weibull with the MLE is not a good fit based on the KS test, which leads to the conclusion that the Weibull with the 'maximum goodness-of-fit estimator' should not be used either. However, this is contrasted with the fact that the KS test does not reject the null hypothesis that the data comes from the Weibull with the scale parameter estimated by the 'maximum goodness-of-fit estimator'. As earlier, the CvM test does not reject any of the null hypotheses. 

So what is the conclusion? These tests show that the one-parameter Weibull may be a reasonable parametric distribution for the data, in some cases. Moreover, we can see that the best choice among the four plotted might be the MME. They cannot be used as a proof that any of these distribution functions are good fits to the data, but they do not reject the possibility of it at least. 

The conclusion is that the Weibull distribution could be an adequate model for this data. Some of the tests reject the null hypothesis, while others do not, so the choice of whether or not to use the Weibull for this data should also be decided based on other factors, such as what distribution is most regularly used in the field of study or what distribution is easiest to work with. But, all in all, I would say that it can be used to model this data, where the most reasonable choice among these four would be the Weibull with the scale parameter estimated with the MME. 

The rest of the problem has been typeset in the other pdf. 

## Problem 2

Two researchers collected data on the price of hardcover textbooks from two disciplinary areas: Mathematics and the Natural Sciences and the Social Sciences. 

```{r, results = "hide"}
books <- read.csv("bookprices.csv")
head(books)
str(books)
summary(books)
books$areaFact <- as.factor(books$Area)
```

### a)

Some exploratory data analysis on book prices for each of the two areas is done. 

```{r}
only.Math <- books[books$Area == "Math & Science", "Price"] # Only Math book prices. 
s.math <- summary(only.Math)
hist(only.Math, breaks = 20, xlab = "Math & Science Books", main = "Histogram of Math & Science Books")

only.Social <- books[books$Area == "Social Sciences", "Price"] # Only Social book prices. 
s.soc <- summary(only.Social)
hist(only.Social, breaks = 20, xlab = "Social Sciences Books", main = "Histogram of Social Sciences Books")

boxplot(Price ~ Area, data = books)

df <- cbind(s.math, s.soc)
colnames(df) <- c("Math & Science", "Social Sciences") 
df <- rbind(df, "Std. Err." = c(sd(only.Math), sd(only.Social)), "Sum" = c(sum(only.Math), sum(only.Social)), 
            "Books" = c(as.integer(length(only.Math)), as.integer(length(only.Social))))
df
```

At first glance at the summary above, the price in the two areas look quite different. This might suggest that there is a difference in prices in general between the two areas. All the quantiles are lower in the Social Sciences compared to the respective quantiles in Math \& Science, which might suggest that prices on many books are lower in the former area compared to the latter area. Note that there are a different amount of books from each area in the data set and that the total sum of prices is different. 

### b) 

A Bootstrap for the mean of book prices in each area is done below. 

```{r}
B <- 10000
b.means.Math <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  b.means.Math[i] <- mean(sample(only.Math, replace = T))
}

boot.mean.Math <- mean(b.means.Math)
boot.sd.Math <- sd(b.means.Math)

rbind("Estimation of Bootstrap Mean" = boot.mean.Math, "Estimation of Bootstrap Std. Err" = boot.sd.Math)
```

```{r}
hist(b.means.Math, breaks = 100, probability = T, main = "Bootstrap Math & Science", xlab = "Esimated Bootstrap Means")
x <- seq(130, 180, by = 1)
lines(x, dnorm(x, mean = boot.mean.Math, sd = boot.sd.Math), col = "blue", lty = 2)
abline(v = boot.mean.Math, col = "red", lty = 1)
legend("topright", legend = c("N(Mean Boot, Std. Err Boot)", "Mean Boot"), col = c("blue", "red"), lty = 2:1)
```

A normal distribution function with the estimated bootstrap mean and standard error is added to the histogram above. Thus, we can see that the distribution of the bootstrap means resembles this normal distribution. 

```{r}
b.means.Social <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  b.means.Social[i] <- mean(sample(only.Social, replace = T))
}
boot.mean.Social <- mean(b.means.Social)
boot.sd.Social <- sd(b.means.Social)

rbind("Estimation of Bootstrap Mean" = boot.mean.Social, "Estimation of Bootstrap Std. Err" = boot.sd.Social)
```

```{r}
hist(b.means.Social, breaks = 100, probability = T, main = "Bootstrap Social Sciences", xlab = "Esimated Bootstrap Means")
x <- seq(30, 160, by = 1)
lines(x, dnorm(x, mean = boot.mean.Social, sd = boot.sd.Social), col = "blue", lty = 2)
abline(v = boot.mean.Social, col = "red", lty = 1)
legend("topright", legend = c("N(Mean Boot, Std. Err Boot)", "Mean Boot"), col = c("blue", "red"), lty = 2:1)
```

Similarly to the case of Math \& Science, a normal distribution function with the estimated bootstrap mean and standard error is added to the histogram above. Thus, we can see that the distribution of the bootstrap means resembles this normal distribution. 

A summary of the bootstrapped means is given below. Also, the summary from a) is plotted again. 

```{r}
df.means <- cbind("Math & Science" = boot.mean.Math, "Social Sciences" = boot.mean.Social)
rownames(df.means) <- "Bootstrap Means"
df.means
df
```

It is apparent that the sample means are relatively similar to the estimations of the bootstrap means. 

Below, the bootstrap has been done using the library `boot`, which gives the same results as when done manually. 

```{r}
mean.func <- function(data, i){
  return(mean(data[i]))
}
set.seed(1)
bm2 <- boot::boot(only.Math, mean.func, R = B)
bm2
mean(bm2$t)

set.seed(1)
bs2 <- boot::boot(only.Social, mean.func, R = B)
bs2
mean(bs2$t)
```

### c) 

The ratio of means is bootstrapped in this problem and an estimate of the standard error of the ratio is provided. 

```{r}
B <- 5000
b.ratio.means <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  Math.resample <- sample(only.Math, replace = T)
  Social.resample <- sample(only.Social, replace = T)
  mean.math <- mean(Math.resample)
  mean.social <- mean(Social.resample)
  b.ratio.means[i] <- mean.math/mean.social 
}
boot.ratio.mean <- mean(b.ratio.means)
boot.ratio.sd <- sd(b.ratio.means)

rbind("Estimation of Bootstrap Ratio of Means" = boot.ratio.mean, "Estimation of Bootstrap Std. Err of Ratio" = boot.ratio.sd)
```

The ratio of means in the sample is `r round(boot.mean.Math/boot.mean.Social, 6)`, which, again, is relatively similar to the estimated bootstrap ratio of means. 

Below, the estimated bootstrap ratio of means is found with the library `boot`. The variances of each of the bootstrap estimation are also estimated, using the bootstrap, such that they can be used to calculate "studentized" confidence intervals in problem d). More specifically, the are found by bootstrapping each resample in the 'outer' bootstrap. Thus we have two levels of bootstrapping: one that resamples from the original sample (to estimate the ratio of means) and one that resamples from each resample (to estimate the variance of each bootstrap estimation). The results are probably different from the results obtained above because of differences in seeds used when sampling in each iteration.

```{r}
N <- 500 # Iterations for Bootstrap estimate of variance. 
new.book.data <- cbind(c(only.Math, only.Social), c(rep(1,times=27), rep(2, times = 17)))
colnames(new.book.data) <- c("Price", "Area")

theta2 <- function(data, i){
  area <- data[i, 2]
  price <- data[i, 1]
  return(mean(price[area==1])/mean(price[area==2]))
}

theta3 <- function(data, i){
  area <- data[i, 2]
  price <- data[i, 1]
  b <- boot::boot(data[i,], theta2, R = N, strata = data[i,2]) 
  #b <- boot::boot(data, theta2, R = N, strata = data[,2]) 
  c(mean(price[area==1])/mean(price[area==2]), var(b$t))
}

set.seed(1)
(b <- boot::boot(new.book.data, theta3, R = B, strata = new.book.data[,2]))
```

The estimated bootstrap ratio of means using the library is 

```{r}
mean(b$t[,1])
```


### d) 

95\% confidence intervals for the ratio of means are found using the "percentile" and the "studentized" method. 

A confidence interval with the "percentile" method is given below. The first interval is the confidence interval for the results when using my manual bootstrap above, and the second interval is the confidence interval calculated with the bootstrap using the library `boot`. 

```{r}
sample.ratio <- mean(only.Math)/mean(only.Social) # Ratio of means in sample.
(ciPercentile <- quantile(b.ratio.means, c(0.025, 0.975))) # Quantile/percentile with manual bootstrap. 
quantile(b$t[,1], c(0.025, 0.975)) # Quantile/percentile with boot library. 
```

A confidence interval with the "studentized" method is given below. 

```{r}
z <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  Math.resample <- sample(only.Math, replace = T)
  Social.resample <- sample(only.Social, replace = T) 
  mean.math <- mean(Math.resample)
  mean.social <- mean(Social.resample)
  b.ratio.means[i] <- mean.math/mean.social  
  new.d <- cbind(c(Math.resample, Social.resample), c(rep(1,times=27), rep(2, times = 17)))
  # Manual bootstrap to estimate the standard error of each bootstrap estimate of the ratio. 
  se.resample <- rep(NA, N)
  for (j in 1:N){
    se.resample[j] <- mean(sample(Math.resample, replace = T))/mean(sample(Social.resample, replace = T))
  }

  # Bootstrap via library to estimate the standard error of each bootstrap estimate of the ratio. 
  #boot.se <- boot::boot(new.d, theta2, R = 1000, strata = new.d[, 2])
  
  # Choice below depends on manual bootstrap or not. 
  #z[i] <- (b.ratio.means[i]-sample.ratio)/sd(boot.se$t)
  z[i] <- (b.ratio.means[i]-sample.ratio)/sd(se.resample)
}

#(ciStudentized <- c(sample.ratio - quantile(z, 0.975)*boot.ratio.sd, sample.ratio - quantile(z, 0.025)*boot.ratio.sd))
(sample.ratio + boot.ratio.sd*quantile(-z, c(0.025, 0.975))) # The same. 
boot.ci(b, type = c("stud", "perc")) 
```

The last output shows the confidence intervals using the `boot.ci` function from the library `boot`. The results when using the library compared to when bootstrapping manually are still a bit different. 

### e) 

A permutation test to compare the two bootstrap means is built in this problem. It is applied to the data and the result is compared with an alternative based on a non-parametric test. First, the difference of bootstrap means is found, which is shown below. 

```{r}
(diff.boot.means <- boot.mean.Math - boot.mean.Social) # First find the difference of means. 
```

In this case, the null hypothesis is that the mean in the two groups are equal. Then, the constructed permutation test is applied to the data. 

```{r}
perms <- 10000
diffs <- rep(NA, perms)
set.seed(1)
for (i in 1:perms){
  s <- sample(books[, "Price"]) # Sample from the combined population.
  diffs[i] <- mean(s[1:27]) - mean(s[28:44]) # Keep 27 in Math and 17 in Social. 
}
```

The histogram shows the distribution of the different permutation resamples. Moreover, the positive difference of means and the negative difference of means are plotted in dashed red lines. 

```{r}
# The null hypothesis looks to not hold that well.
hist(diffs, breaks = 100, main = "Differences of means under null hypothesis (permutations)",
     xlab = "Differences")
abline(v=c(-diff.boot.means, diff.boot.means), col = "red", lty = 2)
```

The $p$-value is found by counting the amount of differences in means found under the null hypothesis, via permutation, that are larger than the positive difference of means or smaller than the negative difference of means. We are using a two-sided test since we want to test the difference between the means in both directions, even though we suspect that the mean in Math \& Science is larger than the mean in Social Sciences. 

```{r}
# Calculate the p-value. Evidence against null hypothesis I would say, which means that the ratios are different. 
(p_value <- (sum(diffs >= diff.boot.means) + sum(diffs <= -diff.boot.means))/perms)
```

As can be seen from the $p$-value above, it is statistically significant to any reasonable level, which means that the null hypothesis can be rejected. We conclude that there exists a difference in the means, based on the two-sided test. What conclusion would a one-sided test yield? Below we calculate the $p$-value only by counting the amount of differences in means under the null hypothesis larger than the positive difference of means from above. 

```{r}
sum(diffs >= diff.boot.means)/perms
```

Again, this value gives rejection of the null hypothesis to a reasonable significance level. Counting the amount of differences in means under the null hypothesis smaller than the negative difference of means from above, as done below, yields the same conclusion. 

```{r}
sum(diffs <= -diff.boot.means)/perms
```

Thus, the permutation test yields the conclusion that the are different. 

**Hvorfor er verdien over enda mindre? I.e. enda mer signifikant! Tenke pÃ¥ null-hypotesene i hvert tilfelle!! Hva betyr det egentlig for konklusjonen av en Ã©n-sidig test? Kanskje bare ha med den to-sidige? **

Next, we should compare this result to alternative based on a non-parametric test. First, a test based on the bootstrap is considered. This is very similar to the permutation test, except that the sampling is with replacement, instead of without replacement. The counted $p$-value is shown below.  

```{r}
B <- 10000
diffs <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  s <- sample(books[, "Price"], replace = T) # Sample from the combined population.
  diffs[i] <- mean(s[1:27]) - mean(s[28:44]) # Keep 27 in Math and 17 in Social. 
}

((sum(diffs >= diff.boot.means) + sum(diffs <= -diff.boot.means))/B)
```

This gives the same conclusion, that the means are statistically different. 

```{r}
# One sided bootstrap. 
(1+sum(diffs >= diff.boot.means))/(B+1)

# One sided bootstrap.
(1+sum(diffs <= -diff.boot.means))/(B+1)
 
# Two sided bootstrap. 2*min(p1, p2) (above)
2*(1+sum(diffs <= -diff.boot.means))/(B+1)

# Can use a pivotal quantity also (as in studentized CI). How is this done!?
#z0 <- (sample.ratio-) # What is \theta_0 on page 23 ?
```

### f) 

Are the answers to the previous questions consistent? The tests done in e) indicate that the means of the two areas are statistically different. The estimate of the bootstrap ratio of the means also indicates the same, since the ratio is estimated to `r boot.ratio.mean` and the standard error of this estimate is estimated to `r boot.ratio.sd`. Additionally, the confidence intervals built from the bootstrap estimates give rise to the same conclusion. This is seen from the confidence intervals, since they do not contain the value 1 (with 95\% confidence), which means that the ratio of means is statistically different from 1. Thus, I would conclude that all answers in this problem are consistent; that the mean price in Math \% Science is larger than the mean price in Social Sciences. 
