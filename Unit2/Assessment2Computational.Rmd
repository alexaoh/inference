---
title: "Computational Part of Assessment Exercises"
subtitle: "Advanced Statistical Inference - Unit 2"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    theme: readable
    highlight: textmate
    number_sections: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/ajo/gitRepos/inference/Unit2")
library(bootstrap)
library(dplyr)
library(boot)
library(resample)
library(perm)
library(dgof)
library(fitdistrplus)
```

## Problem 1

Modelling service times is one area of research in queuing theory. We are given a data set with service times for 174 customers. The distribution of service times is modelled using the one parameter Weibull distribution, which has the pdf

\begin{equation}
  f(x) = \frac{2x}{\theta}e^{-x^2/\theta}I_{(0,\infty)}(x). 
\end{equation}

### a) 

A plot of the empirical cdf overimposed on the theoretical distribution function is shown below. 

```{r, results = "hide"}
data <- read.csv("Service.csv") 
str(data)
head(data)
summary(data)
```


```{r}
x <- seq(0, max(data$Times), by = 0.001) 
Emp.CDF <- ecdf(data$Times)
plot.ecdf(data$Times) 
MLE <- mean(data$Times^2)
MME <- 4*mean(data$Times)^2/pi
lines(x, pweibull(x, shape = 2, scale = sqrt(MLE)), lty = 2, col = "blue", lwd = 3) # MLE estimator for theta.
lines(x, pweibull(x, shape = 2, scale = sqrt(MME)), lty = 3, col = "red", lwd = 3) # MOM-estimator for theta. 
legend("topleft", legend = c("MLE", "MME"), lty = 2:3, col = c("blue", "red"))

fit.weibull <- fitdist(data$Times, distr = "weibull") # the shape parameter should be fixed at 2!
lines(x, pweibull(x, shape = fit.weibull$estimate[[1]], scale = fit.weibull$estimate[[2]]), lty = 2, col = "green", lwd = 3) 
# https://www.rdocumentation.org/packages/fitdistrplus/versions/1.1-6/topics/fitdist
# https://www.google.com/search?channel=fs&client=ubuntu&q=fitdist+weibull+fix+shape+parameter+before+maximization
```

The fit looks alright I suppose. **But how can I overimpose the theoretical distribution on the cdf when I do not know the parameters?**

### b) 

A goodness-of-fit test is done to test whether the Weibull distribution is an adequate model for these data. 
**What goodness-of-fit measure can be used?**

Kolmogorov-Smirnov test can be used (test statistic is like Glivenko-Cantelli (whatever that means!))

```{r}
# Kolmogorov-Smirnov Test for each of the weibulls above. 
MLE.gof <- ks.test(data$Times, "pweibull", shape = 2, scale = sqrt(MLE))
MLE.gof

MME.gof <- ks.test(data$Times, "pweibull", shape = 2, scale = sqrt(MME))
MME.gof

# https://stackoverflow.com/questions/51987605/problems-with-ks-test-and-ties
# https://stats.stackexchange.com/questions/232011/ties-should-not-be-present-in-one-sample-kolmgorov-smirnov-test-in-r
# https://www.rdocumentation.org/packages/dgof/versions/1.2/topics/ks.test
# https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
```


The rest of the problem has been typeset in the other pdf. 

## Problem 2

Two researchers collected data on the price of hardcover textbooks from two disciplinary areas: Mathematics and the Natural Sciences and the Social Sciences. 

```{r, results = "hide"}
books <- read.csv("bookprices.csv")
head(books)
str(books)
summary(books)
books$areaFact <- as.factor(books$Area)
```

### a)

Some exploratory data analysis on book prices for each of the two areas is done. 

```{r}
only.Math <- books[books$Area == "Math & Science", "Price"] # Only Math book prices. 
s.math <- summary(only.Math)
hist(only.Math, breaks = 20, xlab = "Math & Science Books", main = "Histogram of Math & Science Books")

only.Social <- books[books$Area == "Social Sciences", "Price"] # Only Social book prices. 
s.soc <- summary(only.Social)
hist(only.Social, breaks = 20, xlab = "Social Sciences Books", main = "Histogram of Social Sciences Books")

df <- cbind(s.math, s.soc)
colnames(df) <- c("Math & Science", "Social Sciences") 
df <- rbind(df, "Std. Err." = c(sd(only.Math), sd(only.Social)), "Sum" = c(sum(only.Math), sum(only.Social)), 
            "Books" = c(as.integer(length(only.Math)), as.integer(length(only.Social))))
df
```

At first glance at the summary above, the price in the two areas look quite different. This might suggest that there is a difference in prices in general between the two areas. All the quantiles are lower in the Social Sciences compared to Math \& Science, which might suggest that prices on many books are lower in the former area compared to the latter area. Note that there are a different amount of books from each area in the data set and that the total sum of prices is different. 

### b) 

A Bootstrap for the mean of book prices in each area is done below. 

```{r}
B <- 10000
b.means.Math <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  b.means.Math[i] <- mean(sample(only.Math, replace = T))
}

boot.mean.Math <- mean(b.means.Math)
boot.sd.Math <- sd(b.means.Math)

rbind("Estimation of Bootstrap Mean" = boot.mean.Math, "Estimation of Bootstrap Std. Err" = boot.sd.Math)
```

```{r}
hist(b.means.Math, breaks = 100, probability = T, main = "Bootstrap Math & Science", xlab = "Esimated Bootstrap Means")
x <- seq(130, 180, by = 1)
lines(x, dnorm(x, mean = boot.mean.Math, sd = boot.sd.Math), col = "blue", lty = 2)
abline(v = boot.mean.Math, col = "red", lty = 1)
legend("topright", legend = c("N(Mean Boot, Std. Err Boot)", "Mean Boot"), col = c("blue", "red"), lty = 2:1)
```

A normal distribution function with the estimated bootstrap mean and standard error is added to the histogram above. Thus, we can see that the distribution of the bootstrap means resembles this normal distribution. 

```{r}
b.means.Social <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  b.means.Social[i] <- mean(sample(only.Social, replace = T))
}
boot.mean.Social <- mean(b.means.Social)
boot.sd.Social <- sd(b.means.Social)

rbind("Estimation of Bootstrap Mean" = boot.mean.Social, "Estimation of Bootstrap Std. Err" = boot.sd.Social)
```

```{r}
hist(b.means.Social, breaks = 100, probability = T, main = "Bootstrap Social Sciences", xlab = "Esimated Bootstrap Means")
x <- seq(30, 160, by = 1)
lines(x, dnorm(x, mean = boot.mean.Social, sd = boot.sd.Social), col = "blue", lty = 2)
abline(v = boot.mean.Social, col = "red", lty = 1)
legend("topright", legend = c("N(Mean Boot, Std. Err Boot)", "Mean Boot"), col = c("blue", "red"), lty = 2:1)
```

Similarly to the case of Math \& Science, a normal distribution function with the estimated bootstrap mean and standard error is added to the histogram above. Thus, we can see that the distribution of the bootstrap means resembles this normal distribution. 

A summary of the bootstrapped means is given below. Also, the summary from a) is plotted again. 

```{r}
df.means <- cbind("Math & Science" = boot.mean.Math, "Social Sciences" = boot.mean.Social)
rownames(df.means) <- "Bootstrap Means"
df.means
df
```

It is apparent that the sample means are relatively similar to the estimations of the bootstrap means. 

Below the bootstrap has been done with a package, which gives the same results as when done manually. 

```{r}
mean.func <- function(data, i){
  return(mean(data[i]))
}
set.seed(1)
bm2 <- boot::boot(only.Math, mean.func, R = B)
bm2
mean(bm2$t)

set.seed(1)
bs2 <- boot::boot(only.Social, mean.func, R = B)
bs2
mean(bs2$t)
```

### c) 

The ratio of means is bootstrapped in this problem and an estimate of the standard error of the ratio is provided. 

```{r}
B <- 5000
b.ratio.means <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  Math.resample <- sample(only.Math, replace = T)
  Social.resample <- sample(only.Social, replace = T)
  mean.math <- mean(Math.resample)
  mean.social <- mean(Social.resample)
  b.ratio.means[i] <- mean.math/mean.social 
}
boot.ratio.mean <- mean(b.ratio.means)
boot.ratio.sd <- sd(b.ratio.means)

rbind("Estimation of Bootstrap Ratio of Means" = boot.ratio.mean, "Estimation of Bootstrap Std. Err of Ratio" = boot.ratio.sd)
```

The ratio of means in the sample is `r round(boot.mean.Math/boot.mean.Social, 6)`, which, again, is relatively similar to the estimated bootstrap ratio of means. 

Below, the estimated bootstrap ratio of means is found with a library. The variances of each of the bootstrap estimations are also estimated, using the bootstrap, such that they can be used to calculate "studentized" confidence intervals in problem d). The results are probably different from the results obtained above because of differences in seeds used when sampling in each iteration.

```{r}
N <- 500 # Iterations for Bootstrap estimate of variance. 
new.book.data <- cbind(c(only.Math, only.Social), c(rep(1,times=27), rep(2, times = 17)))
colnames(new.book.data) <- c("Price", "Area")

theta2 <- function(data, i){
  area <- data[i, 2]
  price <- data[i, 1]
  return(mean(price[area==1])/mean(price[area==2]))
}

theta3 <- function(data, i){
  area <- data[i, 2]
  price <- data[i, 1]
  #b <- boot::boot(data, theta2, R = N, strata = data[,2]) 
  b <- boot::boot(data[i,], theta2, R = N, strata = data[i,2]) # Her har jeg lagt inn data[i, ]! Husk at dette gir noe annet!
  # Gir dette mening da? TENK! M책 jo resample fra resamplene, s책 synes dette gir mer mening! (usikker p책 formelen i libraryen her,
  # men det stemmer bedre med hva jeg gjore manuelt i funksjonen nedenfor!) Tror dette gir mest mening derfor!
  # Finn ut hvilket svar jeg burde f책 og velge hvilket av alternativene som stemmer deretter!
  c(mean(price[area==1])/mean(price[area==2]), var(b$t))
}

set.seed(1)
(b <- boot::boot(new.book.data, theta3, R = B, strata = new.book.data[,2]))
```

The estimated bootstrap ratio of means with the library is 

```{r}
mean(b$t[,1])
```


### d) 

**Velg den som gir mest mening, og velg tilsvarende valg i theta3 over!**

95\% confidence intervals for the ratio of means are found using the "percentile" and the "studentized" method. 

A confidence interval with the "percentile" method is given below. The first interval is the confidence interval for the results when using my manual bootstrap above, and the second interval is the confidence interval calculated with the bootstrap from the library above. 

```{r}
# Ratio in sample.
sample.ratio <- mean(only.Math)/mean(only.Social) 
(ciPercentile <- quantile(b.ratio.means, c(0.025, 0.975))) # Quantile with manual bootstrap. 
quantile(b$t[,1], c(0.025, 0.975)) # Quantile with boot library. 

c(2*sample.ratio - quantile(b.ratio.means, 0.975), 2*sample.ratio - quantile(b.ratio.means, 0.025)) # Basic with manual bootstrap. 
c(2*sample.ratio - quantile(b$t[,1], 0.975), 2*sample.ratio - quantile(b$t[,1], 0.025)) # Basic with boot library.

# Normal, for comparison. None of them are the same as the one found with boot.ci below! Why?
(ciNorm <- qnorm(p=c(.025, .975), mean=sample.ratio, sd=boot.ratio.sd))
(ciNorm <- qnorm(p=c(.025, .975), mean=sample.ratio, sd=sd(b$t[,1])))
```

A confidence interval with the "studentized" method is given below. 

```{r}
z <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  Math.resample <- sample(only.Math, replace = T)
  Social.resample <- sample(only.Social, replace = T) 
  mean.math <- mean(Math.resample)
  mean.social <- mean(Social.resample)
  b.ratio.means[i] <- mean.math/mean.social  
  new.d <- cbind(c(Math.resample, Social.resample), c(rep(1,times=27), rep(2, times = 17)))
  # Manual bootstrap to estimate the standard error of each bootstrap estimate of the ratio. 
  se.resample <- rep(NA, N)
  for (j in 1:N){
    se.resample[j] <- mean(sample(Math.resample, replace = T))/mean(sample(Social.resample, replace = T))
  }

  # Bootstrap via library to estimate the standard error of each bootstrap estimate of the ratio. 
  #boot.se <- boot::boot(new.book.data, theta2, R = 1000, strata = new.book.data[, 2])
  #boot.se <- boot::boot(new.d, theta2, R = 1000, strata = new.d[, 2])
  # Velg enten new.book.data eller new.d avhengig av hva som velges lenger oppe!
  # Choice below depends on manual bootstrap or not. 
  #z[i] <- (b.ratio.means[i]-sample.ratio)/sd(boot.se$t)
  z[i] <- (b.ratio.means[i]-sample.ratio)/sd(se.resample)
  
  # z calculated by means of the estimated variances in the double-bootstrap from earlier. 
  #z[i] <- (b.ratio.means[i]-sample.ratio)/sqrt(b$t[,2][i])
}

(ciStudentized <- c(sample.ratio - quantile(z, 0.975)*boot.ratio.sd, sample.ratio - quantile(z, 0.025)*boot.ratio.sd))
(sample.ratio + boot.ratio.sd*quantile(-z, c(0.025, 0.975))) # The same. 
boot.ci(b) # This is more similar to the one found via this library. 
# Thus, something wrong in the way the sd is estimated for each bootstrap estimate inside the loop!
```

The results when using the bootstrap package compared to coding the studentized confidence interval manually are quite different. **Surely I am misunderstanding the formula for calculating the studentized CI. Have I used the "correct parts" in the formula above?**

### e) 

A permutation test to compare the two bootstrap means is built in this problem. It is applied to the data and the result is compared with an alternative based on a non-parametric test. First, the difference of bootstrap means is found, which is shown below. 

```{r}
(diff.boot.means <- boot.mean.Math - boot.mean.Social) # First find the difference of means. 
```

Then, the constructed permutation test is applied to the data. 

```{r}
perms <- 10000
diffs <- rep(NA, perms)
set.seed(1)
for (i in 1:perms){
  s <- sample(books[, "Price"]) # Sample from the combined population.
  diffs[i] <- mean(s[1:27]) - mean(s[28:44]) # Keep 27 in Math and 17 in Social. 
}
```

The histogram shows the distribution of the different permutation resamples. Moreover, the positive difference of means and the negative difference of means are plotted in dashed red lines. 

```{r}
# The null hypothesis looks to not hold that well.
hist(diffs, breaks = 100, main = "Differences of means under null hypothesis (permutations)",
     xlab = "Differences")
abline(v=c(-diff.boot.means, diff.boot.means), col = "red", lty = 2)
```

The $p$-value is found by counting the amount of differences in means found under the null hypothesis, via permutation, that are larger than the positive difference of means or smaller than the negative difference of means. We are using a two-sided test since we want to test the difference between the means in both directions, even though we suspect that the mean in Math \& Science is larger than the mean in Social Sciences. 

```{r}
# Calculate the p-value. Evidence against null hypothesis I would say, which means that the ratios are different. 
(p_value <- (sum(diffs >= diff.boot.means) + sum(diffs <= -diff.boot.means))/perms)
```

As can be seen from the $p$-value above, it is statistically significant to any reasonable level, which means that the null hypothesis can be rejected. We conclude that there exists a difference in the means, based on the two-sided test. What conclusion would a one-sided test yield? Below we calculate the $p$-value only by counting the amount of differences in means under the null hypothesis larger than the positive difference of means from above. 

```{r}
sum(diffs >= diff.boot.means)/perms
```

Again, this value gives rejection of the null hypothesis to a reasonable significance level. Counting the amount of differences in means under the null hypothesis smaller than the negative difference of means from above, as done below, yields the same conclusion. 

```{r}
sum(diffs <= -diff.boot.means)/perms
```

Thus, the permutation test yields the conclusion that the are different. 

**Hvorfor er verdien over enda mindre? I.e. enda mer signifikant! Tenke p책 null-hypotesene i hvert tilfelle!! Hva betyr det egentlig for konklusjonen av en 챕n-sidig test? Kanskje bare ha med den to-sidige? **

Next, we should compare this result to alternative based on a non-parametric test. First, a test based on the bootstrap is considered. This is very similar to the permutation test, except that the sampling is with replacement, instead of without replacement. The counted $p$-value is shown below.  

```{r}
B <- 10000
diffs <- rep(NA, B)
set.seed(1)
for (i in 1:B){
  s <- sample(books[, "Price"], replace = T) # Sample from the combined population.
  diffs[i] <- mean(s[1:27]) - mean(s[28:44]) # Keep 27 in Math and 17 in Social. 
}

((sum(diffs >= diff.boot.means) + sum(diffs <= -diff.boot.means))/B)
```

This gives the same conclusion, that the means are statistically different. 

```{r}
# One sided bootstrap. 
(1+sum(diffs >= diff.boot.means))/(B+1)

# One sided bootstrap.
(1+sum(diffs <= -diff.boot.means))/(B+1)
 
# Two sided bootstrap. 2*min(p1, p2) (above)
2*(1+sum(diffs <= -diff.boot.means))/(B+1)

# Can use a pivotal quantity also (as in studentized CI).
#z0 <- (sample.ratio-) # What is \theta_0 on page 23 ?
```

Below, some libraries have been used to show that the conclusion stays the same with different types of non-parametric tests. 

```{r}
# Computed with a library, not sure how it finds Z.
set.seed(1)
p1 <- permTS(b.means.Math, b.means.Social)
p1

# Another non-parametric test. 
t <- wilcox.test(b.means.Math, b.means.Social)
t # This gives the same conclusion at least. 
```


**Som alternativ tests: Kan bruke Wilcox (? (fra stat4BusMan?) Eller en test basert p책 bootstrap? (bootstrap test!)**

### f) 

Are the answers to the previous questions consistent? The tests done in e) indicate that the means of the two areas are statistically different. The estimate of the bootstrap ratio of the means also indicates the same, since the ratio is estimated to `r boot.ratio.mean` and the standard error of this estimate is estimated to `r boot.ratio.sd`. However, some of the confidence intervals built from the bootstrap estimates give rise to the opposite conclusion; that the means are not statistically different. This is seen from the Studentized confidence interval for instance, which contains the value 1 inside the interval, which means that we cannot be confident that the ratio is different from 1. Thus, I would conclude that all answers in this problem are not consistent. **This conclusion depends on which of the two studentized confidence intervals I have as alternatives now (se bilder p책 tlf)!**
